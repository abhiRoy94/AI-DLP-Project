Generative AI apps can unintentionally expose sensitive information, such as source code, financial records, customer data, or personal details, leading to serious risks for businesses and individuals. As the use of these apps become more widespread, the risk grows, especially as users may be unaware of how their data could be leaked to public AI models. This project aims to fine-tune an AI model to recognize sensitive information in both text and speech inputs directed at Gen AI applications.
